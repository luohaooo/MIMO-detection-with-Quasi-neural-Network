{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "from scipy.linalg import orth\n",
    "from sphere_decoding.new_sphereDecodingUseC import sphere_decoding_BER\n",
    "import matplotlib.pyplot as plt\n",
    "# from timeit import default_timer as time\n",
    "\n",
    "# 交叉熵正常训练\n",
    "\n",
    "\n",
    "Nt = 2\n",
    "Nr = 4\n",
    "\n",
    "iter_num = 30\n",
    "channel_list = np.load(\"channel_list_4_2.npy\")\n",
    "H_list = channel_list[0:iter_num]\n",
    "cov_list = np.load(\"covmatrix_list_4.npy\")\n",
    "\n",
    "SNR_list = np.array([0,4,8,12,16])\n",
    "\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# Adam\n",
    "# beta1 = 0.8 \n",
    "# beta2 = 0.999\n",
    "# episilon = 1e-8\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "pilot_length = 128\n",
    "\n",
    "beta1 = 0\n",
    "\n",
    "SD_mean_performance = np.zeros(len(SNR_list))\n",
    "SD_mean_performance_estimated = np.zeros(len(SNR_list))\n",
    "QNN_mean_performance_128 = np.zeros(len(SNR_list))\n",
    "\n",
    "save_loss = np.empty((len(SNR_list), iter_num))\n",
    "save_BER = np.empty((len(SNR_list), iter_num))\n",
    "save_channel = np.empty((len(SNR_list), iter_num, Nr, Nt), dtype=np.complex128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate signals for simulation\n",
    "def generate_random_bit_sequence(length):\n",
    "    return ''.join(random.choice('01') for _ in range(length))\n",
    "\n",
    "def qam16_modulation(binary_input):\n",
    "    mapping = {\n",
    "        '0000': (1+1j),\n",
    "        '0001': (1+3j),\n",
    "        '0010': (3+1j),\n",
    "        '0011': (3+3j),\n",
    "        '0100': (1-1j),\n",
    "        '0101': (1-3j),\n",
    "        '0110': (3-1j),\n",
    "        '0111': (3-3j),\n",
    "        '1000': (-1+1j),\n",
    "        '1001': (-1+3j),\n",
    "        '1010': (-3+1j),\n",
    "        '1011': (-3+3j),\n",
    "        '1100': (-1-1j),\n",
    "        '1101': (-1-3j),\n",
    "        '1110': (-3-1j),\n",
    "        '1111': (-3-3j)\n",
    "    }\n",
    "    return mapping.get(binary_input, \"Invalid binary input\")/np.sqrt(10)\n",
    "\n",
    "def generate_x_sequence(length, Nt):\n",
    "    total_bits_sequence = generate_random_bit_sequence(length*Nt*4)\n",
    "    bits_sequence = [total_bits_sequence[i:i+4] for i in range(0, len(total_bits_sequence), 4)]\n",
    "    x_sequence = np.empty((Nt, length), dtype=np.complex128)\n",
    "    for ii in range(length):\n",
    "        x_sequence[:,ii] = [qam16_modulation(bits_sequence[ii*Nt+jj]) for jj in range(Nt)]\n",
    "    return bits_sequence, x_sequence\n",
    "\n",
    "def generate_channel(SNR):\n",
    "    return np.sqrt(SNR)*H_list\n",
    "\n",
    "def generate_covmatrix(dimension):\n",
    "    U = orth(np.random.randn(dimension,dimension))\n",
    "    x = np.random.rand(dimension)\n",
    "    x = (x/np.sum(x))*dimension\n",
    "    V = np.diag(x)\n",
    "    whitening_matrix = np.dot(U, np.sqrt(V))\n",
    "    cov_matrix = np.dot(whitening_matrix, whitening_matrix.conj().T)\n",
    "    return cov_matrix\n",
    "\n",
    "def generate_noise(cov_matrix,Nr):\n",
    "    real_part = np.random.multivariate_normal(np.zeros(Nr), cov_matrix/2)\n",
    "    imag_part = np.random.multivariate_normal(np.zeros(Nr), cov_matrix/2)\n",
    "    return (real_part+1j*imag_part).reshape(Nr,1)\n",
    "\n",
    "def generate_data(Nr,Nt,length,H_channel,cov_matrix):\n",
    "    bits_sequence, x_sequence = generate_x_sequence(length, Nt)\n",
    "    n_sequence = np.empty((length,Nr,1), dtype=np.complex128)\n",
    "    for ii in range(length):\n",
    "        n_sequence[ii] = generate_noise(cov_matrix,Nr)\n",
    "    y_sequence = np.empty((Nr, length), dtype=np.complex128)\n",
    "    for ii in range(length):\n",
    "        s = np.dot(H_channel, x_sequence[:,ii].reshape(Nt,1))\n",
    "        y_sequence[:, ii] = (s + n_sequence[ii]).reshape(Nr)\n",
    "    return bits_sequence, x_sequence, y_sequence\n",
    "\n",
    "def whiten_matrix(cov_matrix, H):\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    diagmatrix = np.sqrt(np.diag(eigenvalues))\n",
    "    whitening_matrix = np.linalg.inv(np.dot(eigenvectors, diagmatrix))\n",
    "    return np.dot(whitening_matrix, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training H_hat\n",
    "\n",
    "def bits2signals(bits):\n",
    "    # bits: input binary string with length of (4*Nt) \n",
    "    return np.array([qam16_modulation(bits[i:i+4]) for i in range(0, len(bits), 4)]).reshape(Nt,1)\n",
    "\n",
    "def calculate_layer1_training(H_hat, y):\n",
    "    dimension_layer1 = 2**(4*Nt)\n",
    "    # layer1 output\n",
    "    error_norm = np.empty(dimension_layer1)\n",
    "    output = np.empty(dimension_layer1)\n",
    "    # calculate gradient components in layer1\n",
    "    gradients = np.zeros((dimension_layer1, Nr, Nt), dtype=np.complex128)\n",
    "    gradient_component = np.zeros((dimension_layer1, Nr, Nt), dtype=np.complex128)\n",
    "    for index in range(dimension_layer1):\n",
    "        bits = str(bin(index)[2:].zfill(4*Nt))\n",
    "        s = bits2signals(bits)\n",
    "        # s_conjugate_transpose = s.conj().T\n",
    "        y = y.reshape(Nr, 1)\n",
    "        error = y - np.dot(H_hat,s)\n",
    "        error_norm[index] = np.square(np.linalg.norm(error))\n",
    "        gradient_component[index] = np.dot(error, s.conj().T)\n",
    "\n",
    "    min_error_norm = np.min(error_norm)\n",
    "\n",
    "    for index in range(dimension_layer1):\n",
    "        value =  np.exp(-error_norm[index]+min_error_norm)\n",
    "        output[index] = value\n",
    "        gradients[index] = -value*(-gradient_component[index])\n",
    "    # print(output)\n",
    "    return output, gradients\n",
    "\n",
    "\n",
    "def layer2_matrix(n):\n",
    "    if n == 1:\n",
    "        return np.array([0,1])\n",
    "    else:\n",
    "        last_ = layer2_matrix(n-1)\n",
    "        half_cols_num = 2**(n-1)\n",
    "        first_row = np.concatenate((np.zeros(half_cols_num), np.ones(half_cols_num)))\n",
    "        remain_rows = np.hstack((last_, last_))\n",
    "        # print(remain_rows)\n",
    "        return np.vstack((first_row, remain_rows))\n",
    "\n",
    "\n",
    "def calculate_layer2_training(layer1_output, true_output):\n",
    "    total_prob = np.sum(layer1_output)\n",
    "    # print(total_prob)\n",
    "    A = layer2_matrix(4*Nt)\n",
    "    sum_prob_1 = np.dot(A, layer1_output)\n",
    "    # layer2 output\n",
    "    output = sum_prob_1/total_prob\n",
    "    # calculate gradient components in layer2\n",
    "    gradients = np.zeros(2**(4*Nt))\n",
    "    # print(output)\n",
    "    epsilon = 1e-10  # 为了防止log(0)的情况，添加一个小的常数\n",
    "    output = np.clip(output, epsilon, 1. - epsilon)\n",
    "    for ii in range(len(gradients)):\n",
    "        # gradient1 = true_output/output\n",
    "        # gradient2 = (np.ones(len(true_output))-true_output)/(np.ones(len(output))-output)\n",
    "        for jj in range(4*Nt):\n",
    "            gradient1 = true_output[jj]/output[jj]\n",
    "            gradient2 = (1-true_output[jj])/(1-output[jj])\n",
    "            gradient3 = A[jj][ii]/total_prob\n",
    "            gradient4 = sum_prob_1[jj]/np.square(total_prob)\n",
    "            # gradients for cross entropy\n",
    "            gradients[ii] += (-1/(4*Nt))*(gradient1-gradient2)*(gradient3-gradient4)\n",
    "            # gradients for MSE\n",
    "            # gradients[ii] += (1/(4*Nt))*2*(output[jj]-true_output[jj])*(gradient3-gradient4)\n",
    "\n",
    "    return output, gradients\n",
    "\n",
    "\n",
    "def calculate_square_error(layer2_output, true_sequence):\n",
    "    return np.linalg.norm(layer2_output-true_sequence)**2\n",
    "\n",
    "def calculate_cross_entropy(layer2_output, true_sequence):\n",
    "    epsilon = 1e-10  # 为了防止log(0)的情况，添加一个小的常数\n",
    "    layer2_output = np.clip(layer2_output, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -np.mean(true_sequence * np.log(layer2_output) + (1 - true_sequence) * np.log(1 - layer2_output))\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def calculate_cost_function(H_hat):\n",
    "    total_loss = 0\n",
    "    total_gradients = np.zeros((Nr,Nt), dtype=np.complex128)\n",
    "    training_length = len(y_sequence[0])\n",
    "    for ii in range(training_length):\n",
    "        # print(ii)\n",
    "        true_sequence = ''.join(bits_sequence[ii*Nt+jj] for jj in range(Nt))\n",
    "        true_sequence = np.array([eval(ii) for ii in true_sequence])\n",
    "        layer1_output, layer1_gradients = calculate_layer1_training(H_hat, y_sequence[:, ii])\n",
    "        layer2_output, layer2_gradients = calculate_layer2_training(layer1_output, true_sequence)\n",
    "        total_loss += calculate_cross_entropy(layer2_output,true_sequence)\n",
    "        # SGD\n",
    "        if np.random.rand() < 0.9:\n",
    "            for jj in range(2**(4*Nt)):\n",
    "                total_gradients += (layer2_gradients[jj]*layer1_gradients[jj])\n",
    "    mean_loss = total_loss/training_length\n",
    "    return mean_loss, total_gradients\n",
    "\n",
    "\n",
    "def training(max_iter):\n",
    "    H_hat = np.sqrt(1/2)*(np.random.randn(Nr,Nt)+1j*np.random.randn(Nr,Nt))\n",
    "    # H_hat = np.zeros((Nr,Nt), dtype=np.complex128)\n",
    "    # H_hat = np.copy(H_estimated)\n",
    "    momentum = np.zeros((Nr,Nt),dtype=np.complex128)\n",
    "    last_loss = -100\n",
    "    mean_loss = -200\n",
    "    m = np.zeros((Nr,Nt),dtype=np.complex128)\n",
    "    v = np.zeros((Nr,Nt))\n",
    "    for iter_num in range(max_iter):\n",
    "        # solve the gradient\n",
    "        mean_loss, total_gradients = calculate_cost_function(H_hat)\n",
    "        print(\"loss: \"+str(mean_loss))\n",
    "        if np.abs(last_loss-mean_loss) < 1e-4:\n",
    "            return H_hat, mean_loss\n",
    "        else:\n",
    "            last_loss = mean_loss\n",
    "\n",
    "        # update H_hat\n",
    "        momentum = (1-beta1)*total_gradients + beta1*momentum\n",
    "        H_hat -= alpha * momentum\n",
    "\n",
    "        # Adaptive momentum to update H_hat\n",
    "        # m = beta1*m + (1-beta1)*total_gradients # update biased first moment estimate\n",
    "        # gradients_square = np.abs(total_gradients)**2 # elementwise square of gradients matrix\n",
    "        # v = beta2*v + (1-beta2)*gradients_square # update biased second raw moment estimate\n",
    "        # m_hat = m/(1-beta1**(iter_num+1)) # compute bias-corrected first moment estimate\n",
    "        # v_hat = v/(1-beta2**(iter_num+1)) # compute bias-corrected second raw moment estimate\n",
    "        # print(\"v:\"+str(np.sqrt(v_hat)))\n",
    "        # H_hat -= alpha * m_hat / (np.sqrt(v_hat)+episilon) # update H_hat\n",
    "        # print(H_hat)\n",
    "    return H_hat, mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing QNN for detection\n",
    "def calculate_layer1_testing(H_hat, y):\n",
    "    dimension_layer1 = 2**(4*Nt)\n",
    "    # layer1 output\n",
    "    output = np.zeros(dimension_layer1)\n",
    "    error_norm = np.empty(dimension_layer1)\n",
    "    for index in range(dimension_layer1):\n",
    "        bits = str(bin(index)[2:].zfill(4*Nt))\n",
    "        s = bits2signals(bits)\n",
    "        y = y.reshape(Nr,1)\n",
    "        error = y - np.dot(H_hat,s)\n",
    "        error_norm[index] = np.square(np.linalg.norm(error))\n",
    "\n",
    "    min_error_norm = np.min(error_norm)\n",
    "\n",
    "    for index in range(dimension_layer1):\n",
    "        value =  np.exp(-error_norm[index]+min_error_norm)\n",
    "        output[index] = value\n",
    "    return output\n",
    "\n",
    "\n",
    "def calculate_layer2_testing(layer1_output):\n",
    "    total_prob = np.sum(layer1_output)\n",
    "    # print(total_prob)\n",
    "    A = layer2_matrix(4*Nt)\n",
    "    sum_prob_1 = np.dot(A, layer1_output)\n",
    "    # layer2 output\n",
    "    output = np.array([sum_prob_1[ii]/total_prob for ii in range(4*Nt)])\n",
    "    return output\n",
    "\n",
    "def detection(y, H_trained):\n",
    "    layer1_output = calculate_layer1_testing(H_trained, y)\n",
    "    layer2_output = calculate_layer2_testing(layer1_output)\n",
    "    detect_result = ''\n",
    "    for ii in range(len(layer2_output)):\n",
    "        if(layer2_output[ii]>0.5):\n",
    "            detect_result += '1'\n",
    "        else:\n",
    "            detect_result += '0'\n",
    "    return(detect_result)\n",
    "\n",
    "def count_differences(str1, str2):\n",
    "    return sum(a != b for a, b in zip(str1, str2))\n",
    "\n",
    "def calculate_BER(H_trained, bits_sequence_testing, y_sequence_testing):\n",
    "    error = 0\n",
    "    for ii in range(len(y_sequence_testing[0])):\n",
    "        detect_result = detection(y_sequence_testing[:,ii], H_trained)\n",
    "        true_sequence = ''.join(bits_sequence_testing[ii*Nt+jj] for jj in range(Nt))\n",
    "        error += count_differences(detect_result, true_sequence)\n",
    "    BER = error/(len(y_sequence_testing[0])*len(detect_result))\n",
    "    return BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 0\n",
      "SD (estimated CSI): 0.1846923828125\n",
      "loss: 2.100889863611202\n",
      "loss: 0.9503643386496824\n",
      "loss: 1.016722793730889\n",
      "loss: 0.481412932256079\n",
      "loss: 0.3341971174717855\n",
      "loss: 0.3140032960035448\n",
      "loss: 0.30353676224327664\n",
      "loss: 0.2961837833225298\n",
      "loss: 0.2906025631884666\n",
      "loss: 0.28661651219321443\n",
      "loss: 0.2838448467828515\n",
      "loss: 0.28107542074245423\n",
      "loss: 0.2792787385282996\n",
      "loss: 0.27733561717266614\n",
      "loss: 0.27602814765707573\n",
      "loss: 0.2747692326871205\n",
      "loss: 0.2734137344681405\n",
      "loss: 0.2722415625300117\n",
      "loss: 0.2712203402954631\n",
      "loss: 0.2706669846288199\n",
      "loss: 0.2700854757185876\n",
      "loss: 0.269411737286479\n",
      "loss: 0.26874618616997353\n",
      "loss: 0.2682829691792162\n",
      "loss: 0.2678319010713694\n",
      "loss: 0.26758062682517164\n",
      "loss: 0.26699744153808214\n",
      "loss: 0.2666379606619592\n",
      "loss: 0.26655677080759355\n",
      "QNN: 0.1268310546875\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 1\n",
      "SD (estimated CSI): 0.29736328125\n",
      "loss: 1.3403417405505025\n",
      "loss: 1.5353082480613056\n",
      "loss: 1.0609558867989106\n",
      "loss: 0.8334825129298754\n",
      "loss: 1.2748028024956664\n",
      "loss: 0.8106119468721463\n",
      "loss: 0.6356290009717341\n",
      "loss: 0.5537558774413043\n",
      "loss: 0.5013782052211179\n",
      "loss: 0.38817473085109117\n",
      "loss: 0.3049111142952685\n",
      "loss: 0.2739790163501338\n",
      "loss: 0.26709805889884686\n",
      "loss: 0.2631461013650325\n",
      "loss: 0.2608337830026989\n",
      "loss: 0.259183451377846\n",
      "loss: 0.25802607482603884\n",
      "loss: 0.25761259909970696\n",
      "loss: 0.256879197682499\n",
      "loss: 0.25636488580701294\n",
      "loss: 0.25605543874140296\n",
      "loss: 0.2556950539714298\n",
      "loss: 0.2554014595313687\n",
      "loss: 0.25516205641302586\n",
      "loss: 0.25498400585945746\n",
      "loss: 0.2549415534572646\n",
      "QNN: 0.1123046875\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 2\n",
      "SD (estimated CSI): 0.2445068359375\n",
      "loss: 1.068563030871374\n",
      "loss: 0.6983847600428978\n",
      "loss: 0.5831457596927699\n",
      "loss: 0.5317361649982627\n",
      "loss: 0.5076803397734245\n",
      "loss: 0.5015293608812934\n",
      "loss: 0.45604611788665983\n",
      "loss: 0.4125430275072457\n",
      "loss: 0.3874342622612658\n",
      "loss: 0.37368050252788076\n",
      "loss: 0.3651957467507872\n",
      "loss: 0.35972001395832626\n",
      "loss: 0.35623874085043916\n",
      "loss: 0.3533325877937406\n",
      "loss: 0.3509781575278308\n",
      "loss: 0.3497432809068579\n",
      "loss: 0.34825463651481425\n",
      "loss: 0.3471291021142772\n",
      "loss: 0.34624619439987453\n",
      "loss: 0.34565771912504767\n",
      "loss: 0.3457705497775597\n",
      "loss: 0.344935253203351\n",
      "loss: 0.3447312898100117\n",
      "loss: 0.3447502156026609\n",
      "QNN: 0.1558837890625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 3\n",
      "SD (estimated CSI): 0.3040771484375\n",
      "loss: 1.2748325264869198\n",
      "loss: 0.824994476028588\n",
      "loss: 1.0057322606370587\n",
      "loss: 0.7806842768789802\n",
      "loss: 0.42866685329185117\n",
      "loss: 0.3681720219938542\n",
      "loss: 0.35325914152405224\n",
      "loss: 0.3411221632914409\n",
      "loss: 0.33172170466722745\n",
      "loss: 0.3250574758441855\n",
      "loss: 0.3184941854463475\n",
      "loss: 0.31313311135159616\n",
      "loss: 0.3085669341677779\n",
      "loss: 0.30562987584586876\n",
      "loss: 0.30319565976197793\n",
      "loss: 0.30138348609212867\n",
      "loss: 0.3000629995331702\n",
      "loss: 0.2992109095542789\n",
      "loss: 0.2978661099905101\n",
      "loss: 0.29723950623098355\n",
      "loss: 0.2966609158263624\n",
      "loss: 0.2965145785175796\n",
      "loss: 0.29625442280216574\n",
      "loss: 0.2960844176624083\n",
      "loss: 0.2960091003173384\n",
      "QNN: 0.1337890625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 4\n",
      "SD (estimated CSI): 0.3597412109375\n",
      "loss: 0.9063106457868548\n",
      "loss: 0.7236913390915716\n",
      "loss: 0.6759439186553773\n",
      "loss: 0.3921031932476942\n",
      "loss: 0.3361031360044899\n",
      "loss: 0.30990548975447585\n",
      "loss: 0.2949109503142731\n",
      "loss: 0.2844620988429862\n",
      "loss: 0.2768533962377626\n",
      "loss: 0.2708256301780783\n",
      "loss: 0.26645712252093917\n",
      "loss: 0.2628047303632312\n",
      "loss: 0.25972144663816554\n",
      "loss: 0.2566447883802128\n",
      "loss: 0.2543614728328269\n",
      "loss: 0.25203819235602953\n",
      "loss: 0.24997880741434178\n",
      "loss: 0.2483914093716332\n",
      "loss: 0.24711409245775418\n",
      "loss: 0.245782955907608\n",
      "loss: 0.24496932452524223\n",
      "loss: 0.24390042118156283\n",
      "loss: 0.24306053984369702\n",
      "loss: 0.24229148884691626\n",
      "loss: 0.24178737056632352\n",
      "loss: 0.24152725592402682\n",
      "loss: 0.2408768283448677\n",
      "loss: 0.24030183727058074\n",
      "loss: 0.2398568690295289\n",
      "loss: 0.23965350378617614\n",
      "loss: 0.2392844831789232\n",
      "loss: 0.2391328286679478\n",
      "loss: 0.23890340787346348\n",
      "loss: 0.2386716436045886\n",
      "loss: 0.23846236353828354\n",
      "loss: 0.2383305790438305\n",
      "loss: 0.23813998418321278\n",
      "loss: 0.23802745620328106\n",
      "loss: 0.2379482473446814\n",
      "QNN: 0.095947265625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 5\n",
      "SD (estimated CSI): 0.1962890625\n",
      "loss: 2.0552628683928735\n",
      "loss: 1.191858592821628\n",
      "loss: 0.9401513937814443\n",
      "loss: 0.6037855774909372\n",
      "loss: 0.3870547976123715\n",
      "loss: 0.35614463818806513\n",
      "loss: 0.34183317953016146\n",
      "loss: 0.33207619592428955\n",
      "loss: 0.3238772653570453\n",
      "loss: 0.3183192443061603\n",
      "loss: 0.31338392158815537\n",
      "loss: 0.3101668875144502\n",
      "loss: 0.30661652197906786\n",
      "loss: 0.3044477645286648\n",
      "loss: 0.3022862664348517\n",
      "loss: 0.30036965756912387\n",
      "loss: 0.29919122174929896\n",
      "loss: 0.2981868717742798\n",
      "loss: 0.2967426294173518\n",
      "loss: 0.29620580370396965\n",
      "loss: 0.29560008622797174\n",
      "loss: 0.29529361822693634\n",
      "loss: 0.2950630414005382\n",
      "loss: 0.29463359998354544\n",
      "loss: 0.2946698353980627\n",
      "QNN: 0.1365966796875\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 6\n",
      "SD (estimated CSI): 0.2579345703125\n",
      "loss: 1.8425226384285167\n",
      "loss: 0.8662498055953194\n",
      "loss: 0.6803962506487098\n",
      "loss: 0.6413373962688548\n",
      "loss: 0.4697266718968822\n",
      "loss: 0.42925480374900654\n",
      "loss: 0.40703662685361414\n",
      "loss: 0.38837444147800926\n",
      "loss: 0.3747468176122459\n",
      "loss: 0.3643608636514871\n",
      "loss: 0.3556127994797791\n",
      "loss: 0.3494242693215472\n",
      "loss: 0.3448053615228383\n",
      "loss: 0.3414167621225315\n",
      "loss: 0.3394337661001019\n",
      "loss: 0.33798643458438693\n",
      "loss: 0.33719345393548084\n",
      "loss: 0.3362929987892257\n",
      "loss: 0.3361611241792163\n",
      "loss: 0.33541352188183726\n",
      "loss: 0.33497701109663314\n",
      "loss: 0.334999301543946\n",
      "QNN: 0.1514892578125\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 7\n",
      "SD (estimated CSI): 0.2926025390625\n",
      "loss: 1.0570594565644353\n",
      "loss: 1.0307785367884568\n",
      "loss: 0.574140848672608\n",
      "loss: 0.48959854161016964\n",
      "loss: 0.4504886650481667\n",
      "loss: 0.43629142418715094\n",
      "loss: 0.41946890604254183\n",
      "loss: 0.4107137950583753\n",
      "loss: 0.4012726876631454\n",
      "loss: 0.39682144315138485\n",
      "loss: 0.39266546677507846\n",
      "loss: 0.39092601077313044\n",
      "loss: 0.38822250948394715\n",
      "loss: 0.3867961652858416\n",
      "loss: 0.3855365993451063\n",
      "loss: 0.3834443448283534\n",
      "loss: 0.3822973827175519\n",
      "loss: 0.38172803049072146\n",
      "loss: 0.3807187746750469\n",
      "loss: 0.3801600352577281\n",
      "loss: 0.37938777054022305\n",
      "loss: 0.3787066297398313\n",
      "loss: 0.3783008609855503\n",
      "loss: 0.3786728260776313\n",
      "loss: 0.3772165032212801\n",
      "loss: 0.37660847196405567\n",
      "loss: 0.37622768364672454\n",
      "loss: 0.37607058519809927\n",
      "loss: 0.37628310140755267\n",
      "loss: 0.37557911010179584\n",
      "loss: 0.3747022510905267\n",
      "loss: 0.37440752192592197\n",
      "loss: 0.37415975077944574\n",
      "loss: 0.3742490747321079\n",
      "QNN: 0.1722412109375\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 8\n",
      "SD (estimated CSI): 0.277587890625\n",
      "loss: 1.0414498169504631\n",
      "loss: 0.7830062659598451\n",
      "loss: 0.6393341287364835\n",
      "loss: 0.5403025786379916\n",
      "loss: 0.4090765888120655\n",
      "loss: 0.3693413288886141\n",
      "loss: 0.3612848950750335\n",
      "loss: 0.3579367050990715\n",
      "loss: 0.3557639713148586\n",
      "loss: 0.35360275880759673\n",
      "loss: 0.35229038584643135\n",
      "loss: 0.3513422270272698\n",
      "loss: 0.3508988448048366\n",
      "loss: 0.350475857879377\n",
      "loss: 0.3496527803588538\n",
      "loss: 0.3492816755410261\n",
      "loss: 0.3494197902641789\n",
      "loss: 0.3491612668801722\n",
      "loss: 0.34874397257541295\n",
      "loss: 0.3489245214617538\n",
      "loss: 0.3489472188720173\n",
      "QNN: 0.1448974609375\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 9\n",
      "SD (estimated CSI): 0.2093505859375\n",
      "loss: 1.1070839427342838\n",
      "loss: 0.5924997029226413\n",
      "loss: 0.47393338116764416\n",
      "loss: 0.4201998267864455\n",
      "loss: 0.3908526483307504\n",
      "loss: 0.37107643229472\n",
      "loss: 0.3608879608783066\n",
      "loss: 0.353689272715737\n",
      "loss: 0.34997886155930413\n",
      "loss: 0.34745240089510687\n",
      "loss: 0.3462252801766687\n",
      "loss: 0.34489976024898217\n",
      "loss: 0.3445464773969846\n",
      "loss: 0.34393208125076596\n",
      "loss: 0.34379119986168716\n",
      "loss: 0.3433693272441346\n",
      "loss: 0.3432760897165644\n",
      "QNN: 0.1563720703125\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 10\n",
      "SD (estimated CSI): 0.2979736328125\n",
      "loss: 1.5044013543485963\n",
      "loss: 0.809445186671398\n",
      "loss: 0.6102100860709125\n",
      "loss: 0.5318995863885888\n",
      "loss: 0.5034298975992522\n",
      "loss: 0.48592555646348484\n",
      "loss: 0.4676905933410231\n",
      "loss: 0.45369091188288524\n",
      "loss: 0.4437160327904183\n",
      "loss: 0.4357744375528418\n",
      "loss: 0.4292886322014682\n",
      "loss: 0.42425205540210187\n",
      "loss: 0.42104720003386487\n",
      "loss: 0.4175514115571021\n",
      "loss: 0.4148076614475315\n",
      "loss: 0.412117818012926\n",
      "loss: 0.4102683275785406\n",
      "loss: 0.4084748422972722\n",
      "loss: 0.4069809499430912\n",
      "loss: 0.4059166969439236\n",
      "loss: 0.40455953920290644\n",
      "loss: 0.4038433308748271\n",
      "loss: 0.40308676713470293\n",
      "loss: 0.40243127050940086\n",
      "loss: 0.40196219684519735\n",
      "loss: 0.4017848521086381\n",
      "loss: 0.4012225019707308\n",
      "loss: 0.40112598813697264\n",
      "QNN: 0.175048828125\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 11\n",
      "SD (estimated CSI): 0.26806640625\n",
      "loss: 1.0755496657707828\n",
      "loss: 0.7252144762697194\n",
      "loss: 0.45129427122128374\n",
      "loss: 0.3935770319795083\n",
      "loss: 0.36861221135678746\n",
      "loss: 0.3533922868474845\n",
      "loss: 0.3420726169053105\n",
      "loss: 0.3341048730087361\n",
      "loss: 0.32736717295861417\n",
      "loss: 0.32216634186165194\n",
      "loss: 0.31733684621911423\n",
      "loss: 0.31401394323424503\n",
      "loss: 0.31138221401963306\n",
      "loss: 0.30998980921584146\n",
      "loss: 0.3089811297737735\n",
      "loss: 0.3082183344428164\n",
      "loss: 0.3066537809898434\n",
      "loss: 0.3056615276482968\n",
      "loss: 0.30518612124942157\n",
      "loss: 0.3052519629914007\n",
      "QNN: 0.13134765625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 12\n",
      "SD (estimated CSI): 0.3323974609375\n",
      "loss: 1.6685944895662996\n",
      "loss: 1.1540198722897956\n",
      "loss: 0.5786986445125368\n",
      "loss: 0.40972409667174775\n",
      "loss: 0.33137784215574617\n",
      "loss: 0.2861343359721816\n",
      "loss: 0.2565336288429158\n",
      "loss: 0.2304798679942716\n",
      "loss: 0.21324077307063358\n",
      "loss: 0.20195004775529674\n",
      "loss: 0.19543731839417722\n",
      "loss: 0.19042930718197373\n",
      "loss: 0.1868233550464334\n",
      "loss: 0.1842190453730148\n",
      "loss: 0.1820858539975427\n",
      "loss: 0.1801008678666257\n",
      "loss: 0.17907337956693836\n",
      "loss: 0.1775082206898642\n",
      "loss: 0.17655719832160327\n",
      "loss: 0.1756587442821367\n",
      "loss: 0.17470504975057397\n",
      "loss: 0.17416300335300247\n",
      "loss: 0.1735592186468488\n",
      "loss: 0.17305558689371806\n",
      "loss: 0.1725908843804986\n",
      "loss: 0.17236741120736082\n",
      "loss: 0.1719473164044962\n",
      "loss: 0.17179654314585338\n",
      "loss: 0.17132519325160406\n",
      "loss: 0.17114486481441546\n",
      "loss: 0.17098433804158317\n",
      "loss: 0.17097646872946184\n",
      "QNN: 0.0706787109375\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 13\n",
      "SD (estimated CSI): 0.18798828125\n",
      "loss: 0.9902168654733365\n",
      "loss: 0.51372979293497\n",
      "loss: 0.4003014622483048\n",
      "loss: 0.34735757733844974\n",
      "loss: 0.32007240117598185\n",
      "loss: 0.3044992967241283\n",
      "loss: 0.29520973930222394\n",
      "loss: 0.2898384337339749\n",
      "loss: 0.2866289037857936\n",
      "loss: 0.28485608399056406\n",
      "loss: 0.2837088470811808\n",
      "loss: 0.28330500525316415\n",
      "loss: 0.28239614310096645\n",
      "loss: 0.2822170107392007\n",
      "loss: 0.2821046141064005\n",
      "loss: 0.2819709105890085\n",
      "loss: 0.28189400378714496\n",
      "QNN: 0.109375\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 14\n",
      "SD (estimated CSI): 0.3238525390625\n",
      "loss: 1.171547825223432\n",
      "loss: 2.0515883065035907\n",
      "loss: 1.1328717538280741\n",
      "loss: 0.8205236182096781\n",
      "loss: 0.6632864875394451\n",
      "loss: 0.6545946338727439\n",
      "loss: 1.0851033749513352\n",
      "loss: 0.6918947199817863\n",
      "loss: 0.5816842236468296\n",
      "loss: 0.5351330661495648\n",
      "loss: 0.5081213240373538\n",
      "loss: 0.4953936000726825\n",
      "loss: 0.48831606360068225\n",
      "loss: 0.4801466370283804\n",
      "loss: 0.4771856119650027\n",
      "loss: 0.47351567985903414\n",
      "loss: 0.4731481304166595\n",
      "loss: 0.4760962674280022\n",
      "loss: 0.4769759789158749\n",
      "loss: 0.4865539254368499\n",
      "loss: 0.4875209786849491\n",
      "loss: 0.4690808709102011\n",
      "loss: 0.4662016889662852\n",
      "loss: 0.46526313452890977\n",
      "loss: 0.4660218091679233\n",
      "loss: 0.4650342130330077\n",
      "loss: 0.4601866867893954\n",
      "loss: 0.45863819505515\n",
      "loss: 0.4587893428177249\n",
      "loss: 0.4572353902193845\n",
      "loss: 0.45664472477668117\n",
      "loss: 0.45595403511220095\n",
      "loss: 0.4548459318945453\n",
      "loss: 0.4556908495205471\n",
      "loss: 0.45465887732321736\n",
      "loss: 0.4544570078010324\n",
      "loss: 0.4549918537004553\n",
      "loss: 0.4576462338968757\n",
      "loss: 0.45453779707494585\n",
      "loss: 0.45514846844932744\n",
      "loss: 0.45622185903175094\n",
      "loss: 0.4568877292790606\n",
      "loss: 0.4546744294816452\n",
      "loss: 0.45400282804210706\n",
      "loss: 0.45650300606895333\n",
      "loss: 0.4585055944835276\n",
      "loss: 0.46033911609037637\n",
      "loss: 0.4628448984082645\n",
      "loss: 0.4586377336789942\n",
      "loss: 0.46060718636625164\n",
      "loss: 0.463048676569161\n",
      "loss: 0.46710027677380894\n",
      "loss: 0.45747955366038706\n",
      "loss: 0.4613046488464314\n",
      "loss: 0.4587408964617781\n",
      "loss: 0.4624541290673928\n",
      "loss: 0.45913045533772945\n",
      "loss: 0.45715943572491025\n",
      "loss: 0.45434258806379174\n",
      "loss: 0.4522143729150904\n",
      "loss: 0.4520042481292015\n",
      "loss: 0.4532431037209248\n",
      "loss: 0.4542957702515393\n",
      "loss: 0.45374461808025895\n",
      "loss: 0.4563639082424042\n",
      "loss: 0.46370706018479196\n",
      "loss: 0.4769242969556643\n",
      "loss: 0.4704262893180793\n",
      "loss: 0.4736124384911098\n",
      "loss: 0.4665198541647337\n",
      "loss: 0.46238268576524916\n",
      "loss: 0.46157121926999617\n",
      "loss: 0.4558556934355981\n",
      "loss: 0.45412035550425606\n",
      "loss: 0.4532329254591576\n",
      "loss: 0.4518522709716442\n",
      "loss: 0.45284777312603736\n",
      "loss: 0.45265953511045903\n",
      "loss: 0.4514283037452122\n",
      "loss: 0.45266855663195193\n",
      "loss: 0.45485879399992113\n",
      "loss: 0.456451553307555\n",
      "loss: 0.4618847686240829\n",
      "loss: 0.4534942389909867\n",
      "loss: 0.455280530047178\n",
      "loss: 0.4575881532875675\n",
      "loss: 0.4600050529780689\n",
      "loss: 0.4654108508907801\n",
      "loss: 0.45628795703365843\n",
      "loss: 0.4530971400635462\n",
      "loss: 0.451536769063354\n",
      "loss: 0.4531014096336317\n",
      "loss: 0.45211555842463325\n",
      "loss: 0.45161820487993753\n",
      "loss: 0.45225875847221775\n",
      "loss: 0.4513095189554284\n",
      "loss: 0.4516404354528447\n",
      "loss: 0.4511640606140307\n",
      "loss: 0.4517389370533942\n",
      "loss: 0.4538777931729439\n",
      "QNN: 0.232177734375\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 15\n",
      "SD (estimated CSI): 0.2874755859375\n",
      "loss: 1.3819147208660822\n",
      "loss: 0.8903604630714617\n",
      "loss: 0.856412833567443\n",
      "loss: 1.1570622293245556\n",
      "loss: 0.7177428208399627\n",
      "loss: 0.5639024302177649\n",
      "loss: 0.48763283874027663\n",
      "loss: 0.44030314059827574\n",
      "loss: 0.4034020710905795\n",
      "loss: 0.3766592613317717\n",
      "loss: 0.3511144912927144\n",
      "loss: 0.331235881899037\n",
      "loss: 0.3164283172957839\n",
      "loss: 0.3076468221202545\n",
      "loss: 0.30043492787225345\n",
      "loss: 0.29672360356861044\n",
      "loss: 0.29344334791514176\n",
      "loss: 0.290307685046525\n",
      "loss: 0.28876641381018114\n",
      "loss: 0.2871137743797517\n",
      "loss: 0.28640204379834516\n",
      "loss: 0.2856041963972896\n",
      "loss: 0.28544163831004676\n",
      "loss: 0.28496559329656357\n",
      "loss: 0.2850069338993486\n",
      "QNN: 0.1099853515625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 16\n",
      "SD (estimated CSI): 0.2215576171875\n",
      "loss: 1.8451678208123234\n",
      "loss: 0.8617052216596218\n",
      "loss: 0.49188701664348117\n",
      "loss: 0.3666908207811527\n",
      "loss: 0.3193084965143892\n",
      "loss: 0.29283578826944145\n",
      "loss: 0.27565123904842176\n",
      "loss: 0.2655317163853378\n",
      "loss: 0.2591218710574931\n",
      "loss: 0.2555714095578026\n",
      "loss: 0.2534558861876307\n",
      "loss: 0.25216978403573515\n",
      "loss: 0.2512132230814912\n",
      "loss: 0.2507624420568829\n",
      "loss: 0.25026079996666956\n",
      "loss: 0.2501435157149693\n",
      "loss: 0.2498236424236532\n",
      "loss: 0.24960768750219053\n",
      "loss: 0.249511640210863\n",
      "QNN: 0.119140625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 17\n",
      "SD (estimated CSI): 0.2874755859375\n",
      "loss: 0.7471051111085827\n",
      "loss: 1.0932690455077854\n",
      "loss: 0.5633283813649155\n",
      "loss: 0.4658666325623253\n",
      "loss: 0.4324113544245154\n",
      "loss: 0.4297564562840553\n",
      "loss: 0.430940065721445\n",
      "loss: 0.4477775079578366\n",
      "loss: 0.4329292845219747\n",
      "loss: 0.4340759573112883\n",
      "loss: 0.43831612165143635\n",
      "loss: 0.4316136337425668\n",
      "loss: 0.42730148948874874\n",
      "loss: 0.4243025132174779\n",
      "loss: 0.4248055728425558\n",
      "loss: 0.42647921143682843\n",
      "loss: 0.42111837083593756\n",
      "loss: 0.42673513452049644\n",
      "loss: 0.4371988270974144\n",
      "loss: 0.4216675004275485\n",
      "loss: 0.4212744479260133\n",
      "loss: 0.4181842325633074\n",
      "loss: 0.4180002367419322\n",
      "loss: 0.4159736700132648\n",
      "loss: 0.41723726009833617\n",
      "loss: 0.4201331582336779\n",
      "loss: 0.42385147476883234\n",
      "loss: 0.413587918506582\n",
      "loss: 0.4108172580958992\n",
      "loss: 0.40955844786770135\n",
      "loss: 0.409941128851251\n",
      "loss: 0.4113997590625973\n",
      "loss: 0.41103582365183106\n",
      "loss: 0.4091086200952641\n",
      "loss: 0.4092017498907603\n",
      "QNN: 0.1917724609375\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 18\n",
      "SD (estimated CSI): 0.2532958984375\n",
      "loss: 1.7609747875478627\n",
      "loss: 1.5976802121588802\n",
      "loss: 1.7486640439513677\n",
      "loss: 1.6981334430893398\n",
      "loss: 1.5846243864878995\n",
      "loss: 1.5584073048709681\n",
      "loss: 1.5420974448577676\n",
      "loss: 1.3195745173421247\n",
      "loss: 1.1906046951402156\n",
      "loss: 1.4378143424656107\n",
      "loss: 0.7141531772086213\n",
      "loss: 0.7118900484497679\n",
      "loss: 0.700476837830529\n",
      "loss: 0.5548173327856033\n",
      "loss: 0.44943121893137766\n",
      "loss: 0.40666162772634606\n",
      "loss: 0.3849266045761167\n",
      "loss: 0.37304338610374776\n",
      "loss: 0.3648797332746852\n",
      "loss: 0.35999762560414367\n",
      "loss: 0.35673341959747645\n",
      "loss: 0.35427808043522735\n",
      "loss: 0.3522751479401026\n",
      "loss: 0.3507687198329576\n",
      "loss: 0.34920975333476334\n",
      "loss: 0.34828576507456266\n",
      "loss: 0.3473436512516815\n",
      "loss: 0.3462240821148802\n",
      "loss: 0.34522255964290754\n",
      "loss: 0.34436241133513856\n",
      "loss: 0.3434369081364294\n",
      "loss: 0.3429957323196047\n",
      "loss: 0.34247588577492916\n",
      "loss: 0.3416273443320603\n",
      "loss: 0.3410945222891854\n",
      "loss: 0.34128198439670593\n",
      "loss: 0.340254663068006\n",
      "loss: 0.3398011942832789\n",
      "loss: 0.3393548176829553\n",
      "loss: 0.33914469854000456\n",
      "loss: 0.33866055084501645\n",
      "loss: 0.3387464710740931\n",
      "QNN: 0.14453125\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 19\n",
      "SD (estimated CSI): 0.3299560546875\n",
      "loss: 1.5070594207805266\n",
      "loss: 0.9270931453995221\n",
      "loss: 0.7131653460753039\n",
      "loss: 0.6009468685909299\n",
      "loss: 0.6011487222046676\n",
      "loss: 0.4703358602956766\n",
      "loss: 0.4472053528274861\n",
      "loss: 0.4386544114791673\n",
      "loss: 0.4325320970639978\n",
      "loss: 0.42755971833034934\n",
      "loss: 0.42471769971923196\n",
      "loss: 0.4227352347743697\n",
      "loss: 0.42114209456934654\n",
      "loss: 0.4194665895522777\n",
      "loss: 0.4183424916448983\n",
      "loss: 0.41779382507408824\n",
      "loss: 0.4171269890845862\n",
      "loss: 0.4164641085392688\n",
      "loss: 0.4163943127197266\n",
      "QNN: 0.2232666015625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 20\n",
      "SD (estimated CSI): 0.1595458984375\n",
      "loss: 1.8131620276550253\n",
      "loss: 0.8734767330748551\n",
      "loss: 0.5346172240380185\n",
      "loss: 0.4053404695928709\n",
      "loss: 0.35119790683861085\n",
      "loss: 0.3406863319861301\n",
      "loss: 0.33269977553678975\n",
      "loss: 0.3261339255651211\n",
      "loss: 0.321248997923743\n",
      "loss: 0.3161156548142443\n",
      "loss: 0.31182703309697873\n",
      "loss: 0.3084730005957423\n",
      "loss: 0.30599788006949047\n",
      "loss: 0.30301944354333454\n",
      "loss: 0.30053035173068315\n",
      "loss: 0.2983051661555791\n",
      "loss: 0.2963861637352331\n",
      "loss: 0.29475964560601087\n",
      "loss: 0.2932797154769529\n",
      "loss: 0.2917936157095474\n",
      "loss: 0.2905023049755671\n",
      "loss: 0.2897429344017354\n",
      "loss: 0.28844575543478823\n",
      "loss: 0.28748894058979874\n",
      "loss: 0.28661137839477746\n",
      "loss: 0.2856782693719495\n",
      "loss: 0.28493074138101876\n",
      "loss: 0.2842096371706926\n",
      "loss: 0.28364578146126085\n",
      "loss: 0.2829324860329506\n",
      "loss: 0.2824971236033388\n",
      "loss: 0.28194361314744604\n",
      "loss: 0.2815504509713905\n",
      "loss: 0.2810408109974578\n",
      "loss: 0.28121549636976273\n",
      "loss: 0.28038774036257375\n",
      "loss: 0.2800514778331\n",
      "loss: 0.2796764278724622\n",
      "loss: 0.27970761254021204\n",
      "QNN: 0.1275634765625\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 21\n",
      "SD (estimated CSI): 0.2655029296875\n",
      "loss: 1.0985820105781683\n",
      "loss: 0.5332274383734361\n",
      "loss: 0.41117592028524597\n",
      "loss: 0.35971891595911526\n",
      "loss: 0.3285822494955721\n",
      "loss: 0.31079401569049814\n",
      "loss: 0.30110851534900124\n",
      "loss: 0.29467437945024144\n",
      "loss: 0.2909912770272064\n",
      "loss: 0.28823406335836055\n",
      "loss: 0.286842850869006\n",
      "loss: 0.2846611492653862\n",
      "loss: 0.28331065924892523\n",
      "loss: 0.2826271598630886\n",
      "loss: 0.2818819561714778\n",
      "loss: 0.2813102575853501\n",
      "loss: 0.28062127476889953\n",
      "loss: 0.28016017781786934\n",
      "loss: 0.2798792114444832\n",
      "loss: 0.279504868177303\n",
      "loss: 0.2792511566260577\n",
      "loss: 0.27903093181769006\n",
      "loss: 0.2788687938901323\n",
      "loss: 0.27867139576914635\n",
      "loss: 0.27845275478665266\n",
      "loss: 0.27830825134527415\n",
      "loss: 0.2782057607403306\n",
      "loss: 0.2779540630467387\n",
      "loss: 0.2778414871516036\n",
      "loss: 0.2777406986855004\n",
      "loss: 0.27768809312452275\n",
      "QNN: 0.1317138671875\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 22\n",
      "SD (estimated CSI): 0.23681640625\n",
      "loss: 1.287814746272643\n",
      "loss: 0.7390502127214694\n",
      "loss: 0.626487137962061\n",
      "loss: 0.5758072008297813\n",
      "loss: 0.5160523857871142\n",
      "loss: 0.4198481387193486\n",
      "loss: 0.38074588042834817\n",
      "loss: 0.3603897800475421\n",
      "loss: 0.3453796883345554\n",
      "loss: 0.33474605650704575\n",
      "loss: 0.32699145282150494\n",
      "loss: 0.32139242227506365\n",
      "loss: 0.31752122069140243\n",
      "loss: 0.31488736269089285\n",
      "loss: 0.31228553749016585\n",
      "loss: 0.3104571328324419\n",
      "loss: 0.3089624648049065\n",
      "loss: 0.308001478232518\n",
      "loss: 0.30734440933224766\n",
      "loss: 0.3064085354186532\n",
      "loss: 0.30585940862919164\n",
      "loss: 0.30616024104361056\n",
      "loss: 0.3051253730500733\n",
      "loss: 0.30486078997182015\n",
      "loss: 0.30499029185486193\n",
      "loss: 0.3046484476551396\n",
      "loss: 0.30438639126878814\n",
      "loss: 0.3041314423137534\n",
      "loss: 0.3040461575568384\n",
      "QNN: 0.1517333984375\n",
      "----------------------------current SNR_dB: 0\n",
      "----------------------------current iter num: 23\n",
      "SD (estimated CSI): 0.3466796875\n",
      "loss: 1.2853266601337117\n",
      "loss: 0.7260488636426832\n",
      "loss: 0.5976049797011478\n",
      "loss: 0.5397226571238007\n",
      "loss: 0.47659587959888083\n",
      "loss: 0.4340613832580851\n",
      "loss: 0.4064437366506438\n",
      "loss: 0.39115057936375547\n",
      "loss: 0.38029291802607607\n",
      "loss: 0.3743058695244043\n",
      "loss: 0.3686684280446881\n",
      "loss: 0.36459211127602964\n",
      "loss: 0.36141681038284085\n",
      "loss: 0.35884640617410624\n",
      "loss: 0.35700519292576965\n",
      "loss: 0.3557365038369967\n",
      "loss: 0.3540949415199048\n",
      "loss: 0.35291060235636224\n",
      "loss: 0.352128041842333\n",
      "loss: 0.35156550949106014\n",
      "loss: 0.35109360300734915\n",
      "loss: 0.35049085608736336\n",
      "loss: 0.35008874907714593\n",
      "loss: 0.3497596288579627\n",
      "loss: 0.3495281674586023\n",
      "loss: 0.3493432123017957\n",
      "loss: 0.3494200029195178\n"
     ]
    }
   ],
   "source": [
    "for ii in range(len(SNR_list)):\n",
    "    SNR_dB = SNR_list[ii]\n",
    "    SNR = 10**(SNR_dB / 10)\n",
    "    \n",
    "    SD_performance = np.zeros(iter_num)\n",
    "    SD_performance_estimated = np.zeros(iter_num)\n",
    "    QNN_performance_128 = np.zeros(iter_num)\n",
    "\n",
    "    for jj in range(iter_num):\n",
    "        print(\"----------------------------current SNR_dB: \" +str(SNR_dB))\n",
    "        print(\"----------------------------current iter num: \" +str(jj))\n",
    "\n",
    "        H = H_list[jj] * np.sqrt(SNR)\n",
    "        cov = cov_list[0]\n",
    "        # cov = np.eye(Nr)\n",
    "\n",
    "        bits_sequence_testing, x_sequence_testing, y_sequence_testing = generate_data(Nr,Nt,1024,H,cov)\n",
    "        bits_sequence, x_sequence, y_sequence = generate_data(Nr,Nt,pilot_length,H,cov)\n",
    "        H_estimated = np.dot(y_sequence, np.linalg.pinv(x_sequence))\n",
    "        # print(\"估计信道\")\n",
    "        # print(H_estimated)\n",
    "\n",
    "        # SD_performance[jj] = sphere_decoding_BER(H, y_sequence_testing, bits_sequence_testing, 1)\n",
    "        # print(\"SD (perfect CSI): \"+str(SD_performance[jj]))\n",
    "\n",
    "\n",
    "        SD_performance_estimated[jj] = sphere_decoding_BER(H_estimated, y_sequence_testing, bits_sequence_testing, 100000)\n",
    "        print(\"SD (estimated CSI): \"+str(SD_performance_estimated[jj]))\n",
    "\n",
    "        H_w = whiten_matrix(cov, H)\n",
    "        # print(\"白化信道\")\n",
    "        # print(H_w)\n",
    "\n",
    "        H_trained, loss = training(max_iter)\n",
    "        BER = calculate_BER(H_trained, bits_sequence_testing, y_sequence_testing)\n",
    "\n",
    "        # print(\"真实信道\")\n",
    "        # print(H)\n",
    "        # print(\"QNN信道\")\n",
    "        # print(H_trained)\n",
    "        # print(\"白化信道\")\n",
    "        # print(H_w)\n",
    "        \n",
    "\n",
    "        # save_BER[ii][jj] = BER\n",
    "\n",
    "        QNN_performance_128[jj] = BER\n",
    "        print(\"QNN: \"+str(BER))\n",
    "\n",
    "    # SD_mean_performance[ii] = np.mean(SD_performance)\n",
    "    SD_mean_performance_estimated[ii] = np.mean(SD_performance_estimated)\n",
    "    QNN_mean_performance_128[ii] = np.mean(QNN_performance_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33003495, -0.12032008,  0.40484119, -0.15513787],\n",
       "       [-0.12032008,  1.27876119, -0.41453419, -0.06953362],\n",
       "       [ 0.40484119, -0.41453419,  0.8246921 , -0.13861221],\n",
       "       [-0.15513787, -0.06953362, -0.13861221,  1.56651177]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
