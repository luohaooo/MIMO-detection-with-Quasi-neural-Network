{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "from scipy.linalg import orth\n",
    "from sphere_decoding.sphereDecodingUseC import sphere_decoding_BER\n",
    "import matplotlib.pyplot as plt\n",
    "# from timeit import default_timer as time\n",
    "\n",
    "# 交叉熵正常训练\n",
    "\n",
    "\n",
    "Nt = 2\n",
    "Nr = 4\n",
    "\n",
    "iter_num = 30\n",
    "channel_list = np.load(\"channel_list_4_2.npy\")\n",
    "H_list = channel_list[0:iter_num]\n",
    "cov_list = np.load(\"covmatrix_list_4.npy\")\n",
    "\n",
    "SNR_list = np.array([30])\n",
    "\n",
    "\n",
    "alpha = 1e-4\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "pilot_length = 128\n",
    "\n",
    "beta1 = 0\n",
    "\n",
    "SD_mean_performance = np.zeros(len(SNR_list))\n",
    "QNN_mean_performance_128 = np.zeros(len(SNR_list))\n",
    "\n",
    "save_loss = np.empty((len(SNR_list), iter_num))\n",
    "save_BER = np.empty((len(SNR_list), iter_num))\n",
    "save_channel = np.empty((len(SNR_list), iter_num, Nr, Nt), dtype=np.complex128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate signals for simulation\n",
    "def generate_random_bit_sequence(length):\n",
    "    return ''.join(random.choice('01') for _ in range(length))\n",
    "\n",
    "def qam16_modulation(binary_input):\n",
    "    mapping = {\n",
    "        '0000': (1+1j),\n",
    "        '0001': (1+3j),\n",
    "        '0010': (3+1j),\n",
    "        '0011': (3+3j),\n",
    "        '0100': (1-1j),\n",
    "        '0101': (1-3j),\n",
    "        '0110': (3-1j),\n",
    "        '0111': (3-3j),\n",
    "        '1000': (-1+1j),\n",
    "        '1001': (-1+3j),\n",
    "        '1010': (-3+1j),\n",
    "        '1011': (-3+3j),\n",
    "        '1100': (-1-1j),\n",
    "        '1101': (-1-3j),\n",
    "        '1110': (-3-1j),\n",
    "        '1111': (-3-3j)\n",
    "    }\n",
    "    return mapping.get(binary_input, \"Invalid binary input\")/np.sqrt(10)\n",
    "\n",
    "def generate_x_sequence(length, Nt):\n",
    "    total_bits_sequence = generate_random_bit_sequence(length*Nt*4)\n",
    "    bits_sequence = [total_bits_sequence[i:i+4] for i in range(0, len(total_bits_sequence), 4)]\n",
    "    x_sequence = [np.array([qam16_modulation(bits_sequence[i+j]) for j in range(Nt)]) for i in range(0, len(bits_sequence), Nt)]\n",
    "    return bits_sequence, x_sequence\n",
    "\n",
    "def generate_channel(SNR):\n",
    "    return np.sqrt(SNR)*H_list\n",
    "\n",
    "def generate_covmatrix(dimension):\n",
    "    U = orth(np.random.randn(dimension,dimension))\n",
    "    x = np.random.rand(dimension)\n",
    "    x = (x/np.sum(x))*dimension\n",
    "    V = np.diag(x)\n",
    "    whitening_matrix = np.dot(U, np.sqrt(V))\n",
    "    cov_matrix = np.dot(whitening_matrix, whitening_matrix.conj().T)\n",
    "    return cov_matrix\n",
    "\n",
    "def generate_noise(cov_matrix,Nr):\n",
    "    real_part = np.random.multivariate_normal(np.zeros(Nr), cov_matrix/2)\n",
    "    imag_part = np.random.multivariate_normal(np.zeros(Nr), cov_matrix/2)\n",
    "    return (real_part+1j*imag_part).reshape(Nr,1)\n",
    "\n",
    "def generate_data(Nr,Nt,length,H_channel,cov_matrix):\n",
    "    bits_sequence, x_sequence = generate_x_sequence(length, Nt)\n",
    "    n_sequence = np.empty((length,Nr,1), dtype=np.complex128)\n",
    "    for ii in range(length):\n",
    "        n_sequence[ii] = generate_noise(cov_matrix,Nr)\n",
    "    y_sequence = np.empty((length,Nr,1), dtype=np.complex128)\n",
    "    for ii in range(length):\n",
    "        s = np.dot(H_channel, x_sequence[ii].reshape(Nt,1))\n",
    "        y_sequence[ii] = s + n_sequence[ii]\n",
    "    return bits_sequence, x_sequence, y_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training H_hat\n",
    "\n",
    "def bits2signals(bits):\n",
    "    # bits: input binary string with length of (4*Nt) \n",
    "    return np.array([qam16_modulation(bits[i:i+4]) for i in range(0, len(bits), 4)]).reshape(Nt,1)\n",
    "\n",
    "def calculate_layer1_training(H_hat, y):\n",
    "    dimension_layer1 = 2**(4*Nt)\n",
    "    # layer1 output\n",
    "    error_norm = np.empty(dimension_layer1)\n",
    "    output = np.empty(dimension_layer1)\n",
    "    # calculate gradient components in layer1\n",
    "    gradients = np.zeros((dimension_layer1, Nr, Nt), dtype=np.complex128)\n",
    "    gradient_component = np.zeros((dimension_layer1, Nr, Nt), dtype=np.complex128)\n",
    "    for index in range(dimension_layer1):\n",
    "        bits = str(bin(index)[2:].zfill(4*Nt))\n",
    "        s = bits2signals(bits)\n",
    "        # s_conjugate_transpose = s.conj().T\n",
    "        error = y - np.dot(H_hat,s)\n",
    "        error_norm[index] = np.square(np.linalg.norm(error))\n",
    "        gradient_component[index] = np.dot(error, s.conj().T)\n",
    "\n",
    "    min_error_norm = np.min(error_norm)\n",
    "\n",
    "    for index in range(dimension_layer1):\n",
    "        value =  np.exp(-error_norm[index]+min_error_norm)\n",
    "        output[index] = value\n",
    "        gradients[index] = -value*(-gradient_component[index])\n",
    "    # print(output)\n",
    "    return output, gradients\n",
    "\n",
    "\n",
    "def layer2_matrix(n):\n",
    "    if n == 1:\n",
    "        return np.array([0,1])\n",
    "    else:\n",
    "        last_ = layer2_matrix(n-1)\n",
    "        half_cols_num = 2**(n-1)\n",
    "        first_row = np.concatenate((np.zeros(half_cols_num), np.ones(half_cols_num)))\n",
    "        remain_rows = np.hstack((last_, last_))\n",
    "        # print(remain_rows)\n",
    "        return np.vstack((first_row, remain_rows))\n",
    "\n",
    "\n",
    "def calculate_layer2_training(layer1_output, true_output):\n",
    "    total_prob = np.sum(layer1_output)\n",
    "    # print(total_prob)\n",
    "    A = layer2_matrix(4*Nt)\n",
    "    sum_prob_1 = np.dot(A, layer1_output)\n",
    "    # layer2 output\n",
    "    output = sum_prob_1/total_prob\n",
    "    # calculate gradient components in layer2\n",
    "    gradients = np.zeros(2**(4*Nt))\n",
    "    # print(output)\n",
    "    epsilon = 1e-10  # 为了防止log(0)的情况，添加一个小的常数\n",
    "    output = np.clip(output, epsilon, 1. - epsilon)\n",
    "    for ii in range(len(gradients)):\n",
    "        # gradient1 = true_output/output\n",
    "        # gradient2 = (np.ones(len(true_output))-true_output)/(np.ones(len(output))-output)\n",
    "        for jj in range(4*Nt):\n",
    "            gradient1 = true_output[jj]/output[jj]\n",
    "            gradient2 = (1-true_output[jj])/(1-output[jj])\n",
    "            gradient3 = A[jj][ii]/total_prob\n",
    "            gradient4 = sum_prob_1[jj]/np.square(total_prob)\n",
    "            # gradients for cross entropy\n",
    "            gradients[ii] += (-1/(4*Nt))*(gradient1-gradient2)*(gradient3-gradient4)\n",
    "            # gradients for MSE\n",
    "            # gradients[ii] += (1/(4*Nt))*2*(output[jj]-true_output[jj])*(gradient3-gradient4)\n",
    "\n",
    "    return output, gradients\n",
    "\n",
    "\n",
    "def calculate_square_error(layer2_output, true_sequence):\n",
    "    return np.linalg.norm(layer2_output-true_sequence)**2\n",
    "\n",
    "def calculate_cross_entropy(layer2_output, true_sequence):\n",
    "    epsilon = 1e-10  # 为了防止log(0)的情况，添加一个小的常数\n",
    "    layer2_output = np.clip(layer2_output, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -np.mean(true_sequence * np.log(layer2_output) + (1 - true_sequence) * np.log(1 - layer2_output))\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def calculate_cost_function(H_hat):\n",
    "    total_loss = 0\n",
    "    total_gradients = np.zeros((Nr,Nt), dtype=np.complex128)\n",
    "    training_length = len(y_sequence)\n",
    "    for ii in range(training_length):\n",
    "        # print(ii)\n",
    "        true_sequence = ''.join(bits_sequence[ii*Nt+jj] for jj in range(Nt))\n",
    "        true_sequence = np.array([eval(ii) for ii in true_sequence])\n",
    "        layer1_output, layer1_gradients = calculate_layer1_training(H_hat, y_sequence[ii])\n",
    "        layer2_output, layer2_gradients = calculate_layer2_training(layer1_output, true_sequence)\n",
    "        total_loss += calculate_cross_entropy(layer2_output,true_sequence)\n",
    "        # SGD\n",
    "        if np.random.rand() < 0.9:\n",
    "            for jj in range(2**(4*Nt)):\n",
    "                total_gradients += (layer2_gradients[jj]*layer1_gradients[jj])\n",
    "    mean_loss = total_loss/training_length\n",
    "    return mean_loss, total_gradients\n",
    "\n",
    "\n",
    "def training(max_iter):\n",
    "    # H_hat = np.sqrt(1/2)*(np.random.randn(Nr,Nt)+1j*np.random.randn(Nr,Nt))\n",
    "    H_hat = np.zeros((Nr,Nt), dtype=np.complex128)\n",
    "    # H_hat = np.copy(H_w)\n",
    "    momentum = np.zeros((Nr,Nt),dtype=np.complex128)\n",
    "    last_loss = -100\n",
    "    mean_loss = -200\n",
    "    for iter_num in range(max_iter):\n",
    "        # solve the gradient\n",
    "        mean_loss, total_gradients = calculate_cost_function(H_hat)\n",
    "        print(\"loss: \"+str(mean_loss))\n",
    "        if np.abs(last_loss-mean_loss) < 0.001:\n",
    "            return H_hat, mean_loss\n",
    "        else:\n",
    "            last_loss = mean_loss\n",
    "        # update H_hat\n",
    "        momentum = (1-beta1)*total_gradients + beta1*momentum\n",
    "        H_hat -= alpha * momentum\n",
    "        # print(H_hat)\n",
    "    return H_hat, mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing QNN for detection\n",
    "def calculate_layer1_testing(H_hat, y):\n",
    "    dimension_layer1 = 2**(4*Nt)\n",
    "    # layer1 output\n",
    "    output = np.zeros(dimension_layer1)\n",
    "    error_norm = np.empty(dimension_layer1)\n",
    "    for index in range(dimension_layer1):\n",
    "        bits = str(bin(index)[2:].zfill(4*Nt))\n",
    "        s = bits2signals(bits)\n",
    "        error = y - np.dot(H_hat,s)\n",
    "        error_norm[index] = np.square(np.linalg.norm(error))\n",
    "\n",
    "    min_error_norm = np.min(error_norm)\n",
    "\n",
    "    for index in range(dimension_layer1):\n",
    "        value =  np.exp(-error_norm[index]+min_error_norm)\n",
    "        output[index] = value\n",
    "    return output\n",
    "\n",
    "\n",
    "def calculate_layer2_testing(layer1_output):\n",
    "    total_prob = np.sum(layer1_output)\n",
    "    # print(total_prob)\n",
    "    A = layer2_matrix(4*Nt)\n",
    "    sum_prob_1 = np.dot(A, layer1_output)\n",
    "    # layer2 output\n",
    "    output = np.array([sum_prob_1[ii]/total_prob for ii in range(4*Nt)])\n",
    "    return output\n",
    "\n",
    "def detection(y, H_trained):\n",
    "    layer1_output = calculate_layer1_testing(H_trained, y)\n",
    "    layer2_output = calculate_layer2_testing(layer1_output)\n",
    "    detect_result = ''\n",
    "    for ii in range(len(layer2_output)):\n",
    "        if(layer2_output[ii]>0.5):\n",
    "            detect_result += '1'\n",
    "        else:\n",
    "            detect_result += '0'\n",
    "    return(detect_result)\n",
    "\n",
    "def count_differences(str1, str2):\n",
    "    return sum(a != b for a, b in zip(str1, str2))\n",
    "\n",
    "def calculate_BER(H_trained, bits_sequence_testing, y_sequence_testing):\n",
    "    error = 0\n",
    "    for ii in range(len(y_sequence_testing)):\n",
    "        detect_result = detection(y_sequence_testing[ii], H_trained)\n",
    "        true_sequence = ''.join(bits_sequence_testing[ii*Nt+jj] for jj in range(Nt))\n",
    "        error += count_differences(detect_result, true_sequence)\n",
    "    BER = error/(len(y_sequence_testing)*len(detect_result))\n",
    "    return BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------current SNR_dB: 30\n",
      "----------------------------current iter num: 0\n",
      "SD: 0.0\n",
      "loss: 0.6931471805599462\n",
      "loss: 0.7983570584067438\n",
      "loss: 1.9923868093037096\n",
      "loss: 1.948942715473663\n",
      "loss: 0.5036515160071756\n",
      "loss: 0.48604613727749124\n",
      "loss: 0.6862809813205898\n",
      "loss: 1.445733474801237\n",
      "loss: 1.0327164558517643\n",
      "loss: 0.7126445230797439\n",
      "loss: 0.5729909034160005\n",
      "loss: 0.44615968973233827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m H_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(H)\n\u001b[0;32m     21\u001b[0m bits_sequence, x_sequence, y_sequence \u001b[38;5;241m=\u001b[39m generate_data(Nr,Nt,pilot_length,H,cov)\n\u001b[1;32m---> 22\u001b[0m H_trained, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m BER \u001b[38;5;241m=\u001b[39m calculate_BER(H_trained, bits_sequence_testing, y_sequence_testing)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# save_BER[ii][jj] = BER\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[101], line 111\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(max_iter)\u001b[0m\n\u001b[0;32m    108\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# solve the gradient\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     mean_loss, total_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_cost_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(mean_loss))\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs(last_loss\u001b[38;5;241m-\u001b[39mmean_loss) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.001\u001b[39m:\n",
      "Cell \u001b[1;32mIn[101], line 91\u001b[0m, in \u001b[0;36mcalculate_cost_function\u001b[1;34m(H_hat)\u001b[0m\n\u001b[0;32m     89\u001b[0m true_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(bits_sequence[ii\u001b[38;5;241m*\u001b[39mNt\u001b[38;5;241m+\u001b[39mjj] \u001b[38;5;28;01mfor\u001b[39;00m jj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Nt))\n\u001b[0;32m     90\u001b[0m true_sequence \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28meval\u001b[39m(ii) \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m true_sequence])\n\u001b[1;32m---> 91\u001b[0m layer1_output, layer1_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_layer1_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sequence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m layer2_output, layer2_gradients \u001b[38;5;241m=\u001b[39m calculate_layer2_training(layer1_output, true_sequence)\n\u001b[0;32m     93\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m calculate_cross_entropy(layer2_output,true_sequence)\n",
      "Cell \u001b[1;32mIn[101], line 20\u001b[0m, in \u001b[0;36mcalculate_layer1_training\u001b[1;34m(H_hat, y)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# s_conjugate_transpose = s.conj().T\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     error \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(H_hat,s)\n\u001b[1;32m---> 20\u001b[0m     error_norm[index] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m     gradient_component[index] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(error, s\u001b[38;5;241m.\u001b[39mconj()\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     23\u001b[0m min_error_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(error_norm)\n",
      "File \u001b[1;32mc:\\Users\\29265\\anaconda3\\envs\\py39\\lib\\site-packages\\numpy\\linalg\\linalg.py:2550\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2548\u001b[0m     x_real \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m   2549\u001b[0m     x_imag \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mimag\n\u001b[1;32m-> 2550\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x_real\u001b[38;5;241m.\u001b[39mdot(x_real) \u001b[38;5;241m+\u001b[39m \u001b[43mx_imag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_imag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2552\u001b[0m     sqnorm \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdot(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ii in range(len(SNR_list)):\n",
    "    SNR_dB = SNR_list[ii]\n",
    "    SNR = 10**(SNR_dB / 10)\n",
    "    \n",
    "    SD_performance = np.zeros(iter_num)\n",
    "    QNN_performance_128 = np.zeros(iter_num)\n",
    "\n",
    "    for jj in range(iter_num):\n",
    "        print(\"----------------------------current SNR_dB: \" +str(SNR_dB))\n",
    "        print(\"----------------------------current iter num: \" +str(jj))\n",
    "\n",
    "        H = H_list[jj] * np.sqrt(SNR)\n",
    "        cov = cov_list[0]\n",
    "\n",
    "        bits_sequence_testing, x_sequence_testing, y_sequence_testing = generate_data(Nr,Nt,1024,H,cov)\n",
    "        SD_performance[jj] = sphere_decoding_BER(H, y_sequence_testing, bits_sequence_testing, 1)\n",
    "        print(\"SD: \"+str(SD_performance[jj]))\n",
    "\n",
    "        H_w = np.copy(H)\n",
    "\n",
    "        bits_sequence, x_sequence, y_sequence = generate_data(Nr,Nt,pilot_length,H,cov)\n",
    "        H_trained, loss = training(max_iter)\n",
    "        \n",
    "        \n",
    "        BER = calculate_BER(H_trained, bits_sequence_testing, y_sequence_testing)\n",
    "\n",
    "        # save_BER[ii][jj] = BER\n",
    "\n",
    "        QNN_performance_128[jj] = BER\n",
    "        print(\"QNN: \"+str(BER))\n",
    "\n",
    "    SD_mean_performance[ii] = np.mean(SD_performance)\n",
    "    QNN_mean_performance_128[ii] = np.mean(QNN_performance_128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
