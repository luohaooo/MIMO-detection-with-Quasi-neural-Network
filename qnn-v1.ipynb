{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# generate training and tesing data\n",
    "Nt = 2\n",
    "Nr = 4\n",
    "# generate channel\n",
    "H = np.sqrt(1/2)*(np.random.randn(Nr,Nt)+1j*np.random.randn(Nr,Nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate transmit signal\n",
    "def generate_random_bit_sequence(length):\n",
    "    return ''.join(random.choice('01') for _ in range(length))\n",
    "\n",
    "def qam16_modulation(binary_input):\n",
    "    mapping = {\n",
    "        '0000': (1+1j),\n",
    "        '0001': (1+3j),\n",
    "        '0010': (3+1j),\n",
    "        '0011': (3+3j),\n",
    "        '0100': (1-1j),\n",
    "        '0101': (1-3j),\n",
    "        '0110': (3-1j),\n",
    "        '0111': (3-3j),\n",
    "        '1000': (-1+1j),\n",
    "        '1001': (-1+3j),\n",
    "        '1010': (-3+1j),\n",
    "        '1011': (-3+3j),\n",
    "        '1100': (-1-1j),\n",
    "        '1101': (-1-3j),\n",
    "        '1110': (-3-1j),\n",
    "        '1111': (-3-3j)\n",
    "    }\n",
    "    return mapping.get(binary_input, \"Invalid binary input\")/np.sqrt(10)\n",
    "\n",
    "def generate_x_sequence(length, Nt):\n",
    "    total_bits_sequence = generate_random_bit_sequence(length*Nt*4)\n",
    "    bits_sequence = [total_bits_sequence[i:i+4] for i in range(0, len(total_bits_sequence), 4)]\n",
    "    x_sequence = [np.array([qam16_modulation(bits_sequence[i+j]) for j in range(Nt)]) for i in range(0, len(bits_sequence), Nt)]\n",
    "    return bits_sequence, x_sequence\n",
    "\n",
    "# noise\n",
    "SNR_dB = 0\n",
    "SNR = 10.0**(SNR_dB/10.0)\n",
    "def generate_noise(SNR, Nr):\n",
    "    return np.sqrt(1/(2*SNR))*(np.random.randn(Nr,1)+1j*np.random.randn(Nr,1))\n",
    "\n",
    "# generate training and tesing data\n",
    "def generate_data(Nr,Nt,SNR,length,H_channel):\n",
    "    bits_sequence, x_sequence = generate_x_sequence(length, Nt)\n",
    "    n_sequence = [generate_noise(SNR, Nr) for i in range(length)]\n",
    "    y_sequence = [np.dot(H_channel, x_sequence[i].reshape(Nt,1)) + n_sequence[i] for i in range(length)]\n",
    "    return bits_sequence, x_sequence, y_sequence\n",
    "\n",
    "training_length = 16\n",
    "bits_sequence, x_sequence, y_sequence = generate_data(Nr,Nt,SNR,training_length,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits2signals(bits):\n",
    "    # bits: input binary string with length of (4*Nt) \n",
    "    return np.array([qam16_modulation(bits[i:i+4]) for i in range(0, len(bits), 4)]).reshape(Nt,1)\n",
    "def calculate_layer1(H_hat, y):\n",
    "    dimension_layer1 = 2**(4*Nt)\n",
    "    output = {}\n",
    "    for index in range(dimension_layer1):\n",
    "        bits = str(bin(index)[2:].zfill(4*Nt))\n",
    "        s = bits2signals(bits)\n",
    "        error = y - np.dot(H_hat,s)\n",
    "        value =  np.exp(-np.square(np.linalg.norm(error)))\n",
    "        output[bits] = value\n",
    "    return output\n",
    "\n",
    "def calculate_layer2(layer1_output):\n",
    "    sum_exp = [[0 for i in range(2)] for j in range(4*Nt)]\n",
    "    for bits in layer1_output:\n",
    "        value = layer1_output[bits]\n",
    "        for index in range(4*Nt):\n",
    "            sum_exp[index][eval(bits[index])] += value\n",
    "    output = {}\n",
    "    for index in range(4*Nt):\n",
    "        # llr = np.log(sum_exp[index][1]/sum_exp[index][0])\n",
    "        output[index] = (sum_exp[index][1])/(sum_exp[index][1]+sum_exp[index][0])\n",
    "    return output\n",
    "\n",
    "def calculate_cross_entropy(layer2_output, true_sequence):\n",
    "    dimension = len(true_sequence)\n",
    "    entropy = 0\n",
    "    for index in range(dimension):\n",
    "        if true_sequence[index] == '1':\n",
    "            entropy += (-np.log(layer2_output[index]))\n",
    "    return entropy\n",
    "\n",
    "def calculate_square_error(layer2_output, true_sequence):\n",
    "    dimension = len(true_sequence)\n",
    "    loss = 0\n",
    "    for index in range(dimension):\n",
    "        if true_sequence[index] == '1':\n",
    "            loss += np.square(1-layer2_output[index])\n",
    "        else:\n",
    "            loss += np.square(layer2_output[index])\n",
    "    return loss\n",
    "\n",
    "def calculate_cost_function(H_hat_vec):\n",
    "    H_hat = H_hat_vec[0:Nr*Nt].reshape(Nr,Nt)+1j*H_hat_vec[Nr*Nt:2*Nr*Nt].reshape(Nr,Nt)\n",
    "    # H_hat = H_hat_vec\n",
    "    total_loss = 0\n",
    "    for ii in range(training_length):\n",
    "        layer1_output = calculate_layer1(H_hat, y_sequence[ii])\n",
    "        layer2_output = calculate_layer2(layer1_output)\n",
    "        true_sequence = ''.join(bits_sequence[ii*Nt+jj] for jj in range(Nt))\n",
    "        total_loss += calculate_square_error(layer2_output,true_sequence)\n",
    "    mean_loss = total_loss/training_length\n",
    "    print(mean_loss)\n",
    "    return mean_loss\n",
    "        \n",
    "# calculate_cost_function(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(y, H_trained):\n",
    "    layer1_output = calculate_layer1(H_trained, y)\n",
    "    layer2_output = calculate_layer2(layer1_output)\n",
    "    detect_result = ''\n",
    "    for ii in range(len(layer2_output)):\n",
    "        if(layer2_output[ii]>0.5):\n",
    "            detect_result += '1'\n",
    "        else:\n",
    "            detect_result += '0'\n",
    "    return(detect_result)\n",
    "\n",
    "def count_differences(str1, str2):\n",
    "    return sum(a != b for a, b in zip(str1, str2))\n",
    "\n",
    "def calculate_BER(H_trained):\n",
    "    # tesing set\n",
    "    testing_length = 1000\n",
    "    bits_sequence_testing, x_sequence_testing, y_sequence_testing = generate_data(Nr,Nt,SNR,testing_length,H)\n",
    "    error = 0\n",
    "    for ii in range(len(y_sequence_testing)):\n",
    "        detect_result = detection(y_sequence_testing[ii], H_trained)\n",
    "        true_sequence = ''.join(bits_sequence_testing[ii*Nt+jj] for jj in range(Nt))\n",
    "        error += count_differences(detect_result, true_sequence)\n",
    "    BER = error/(len(y_sequence_testing)*len(detect_result))\n",
    "    return BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.01427566228821\n",
      "3.155523145625767\n",
      "2.9959791818769874\n",
      "3.308101677840054\n",
      "3.187016007811539\n",
      "3.2222492794902964\n",
      "2.863934210426205\n",
      "3.1447101666815898\n",
      "2.9516961249761633\n",
      "2.7572114597434716\n",
      "2.855480988364436\n",
      "2.9647917490631794\n",
      "2.437959982231955\n",
      "2.497915294751345\n",
      "2.4531557312354733\n",
      "2.5932813250932303\n",
      "2.4855936878361797\n",
      "2.2990450312154485\n",
      "2.394026568619216\n",
      "2.1761908717072447\n",
      "2.3440307498813255\n",
      "2.3211463658356855\n",
      "2.099321954344559\n",
      "2.2102328267476756\n",
      "2.38463590307817\n",
      "2.169163330283286\n",
      "2.2738978792880804\n",
      "2.1493710324522\n",
      "2.3489884592283894\n",
      "2.105098050727789\n",
      "2.262830844909533\n",
      "2.107885989981725\n",
      "2.0403353413385714\n",
      "2.0240091519229986\n",
      "2.0133669462357573\n",
      "2.0594112540378027\n",
      "2.11323507584585\n",
      "2.061298480144549\n",
      "2.0284046049734537\n",
      "2.0539963654931284\n",
      "2.0598698814701337\n",
      "1.9356309855399672\n",
      "1.9256408236814018\n",
      "1.8983288613707643\n",
      "1.8935802990036024\n",
      "1.9062158885573854\n",
      "2.108413183487791\n",
      "1.9347204986657607\n",
      "1.9038437286992327\n",
      "1.8917021866599901\n",
      "1.9410552454354277\n",
      "1.9394049990383495\n",
      "1.9109960668677939\n",
      "1.8689710157497528\n",
      "1.9379639898732315\n",
      "1.8947978322194303\n",
      "1.9212103819044364\n",
      "1.9235195777860157\n",
      "2.0029331068174687\n",
      "1.8685242024550286\n",
      "1.8993130286812883\n",
      "1.8453012900825443\n",
      "1.9767335070769183\n",
      "1.8639701478990691\n",
      "1.9115292798956471\n",
      "1.8715333600895379\n",
      "1.8291924957334758\n",
      "1.833950937209577\n",
      "1.8164271448215572\n",
      "1.8345923994331268\n",
      "1.8275265944489776\n",
      "1.7808645101159104\n",
      "1.772516266187627\n",
      "1.8015912625337664\n",
      "1.7655398466569816\n",
      "1.7595038581903688\n",
      "1.7702212702647542\n",
      "1.7777762845739076\n",
      "1.7642012266586502\n",
      "1.7770954346309504\n",
      "1.7709453995582936\n",
      "1.7461377453875258\n",
      "1.7532214491469202\n",
      "1.7298589254533636\n",
      "1.7365791522697638\n",
      "1.7463665591770998\n",
      "1.780013224946646\n",
      "1.728492269132941\n",
      "1.7404076997288271\n",
      "1.7379137411776837\n",
      "1.7158734726916038\n",
      "1.761301407558726\n",
      "1.7077781046834684\n",
      "1.7142709117805768\n",
      "1.7087775177920979\n",
      "1.7223772296565716\n",
      "1.706065671356262\n",
      "1.7420258421108978\n",
      "1.7127170314616973\n",
      "1.7338743320308234\n",
      "1.7146459069036117\n",
      "1.7716017411730642\n",
      "1.7147519602574506\n",
      "1.7042139525381557\n",
      "1.6942742141158975\n",
      "1.6956148914517672\n",
      "1.6991161755679607\n",
      "1.7071270957390765\n",
      "1.6887165003867688\n",
      "1.6938152616925408\n",
      "1.6713317597450108\n",
      "1.6790045382952794\n",
      "1.6715838096298683\n",
      "1.6677738872162855\n",
      "1.6736223060101783\n",
      "1.6553137863363503\n",
      "1.654826853225521\n",
      "1.6554282496591033\n",
      "1.6497332150771133\n",
      "1.6480862166465942\n",
      "1.6444317915261353\n",
      "1.65315137156976\n",
      "1.6502149550619671\n",
      "1.6439699375890335\n",
      "1.6275935838825335\n",
      "1.6104953790401015\n",
      "1.6121029244246767\n",
      "1.618921482029227\n",
      "1.619670379365503\n",
      "1.6107722662495187\n",
      "1.5985211891804938\n",
      "1.6075323210892822\n",
      "1.5902710538057834\n",
      "1.5939601502499272\n",
      "1.590673026755103\n",
      "1.5910419358220584\n",
      "1.5842658681574093\n",
      "1.586014776196107\n",
      "1.5906838844272362\n",
      "1.5858165205210428\n",
      "1.6034920424150898\n",
      "1.5796590347274184\n",
      "1.5804552804091723\n",
      "1.5867960780337196\n",
      "1.5713780914897408\n",
      "1.5671406344987844\n",
      "1.5637599400114057\n",
      "1.5689877455611192\n",
      "1.5679641886500315\n",
      "1.5689765787145806\n",
      "1.566639275649293\n",
      "1.5634356249738046\n",
      "1.5713820809054981\n",
      "1.5654899675384777\n",
      "1.5742069248932602\n",
      "1.5652498926072165\n",
      "1.5731720589672893\n",
      "1.555077648608569\n",
      "1.5536968333274275\n",
      "1.5526492782083248\n",
      "1.5500080377967735\n",
      "1.542960431721131\n",
      "1.5798112922513892\n",
      "1.5462308312987127\n",
      "1.544949573628272\n",
      "1.5523795631228077\n",
      "1.5345971900780035\n",
      "1.5355622198416004\n",
      "1.533710661574894\n",
      "1.5429019908369168\n",
      "1.5457918971965041\n",
      "1.5282909295432017\n",
      "1.5261771455694344\n",
      "1.5226352319608243\n",
      "1.5592581424223018\n",
      "1.5114478346283655\n",
      "1.499548430691645\n",
      "1.5096738586886218\n",
      "1.4907549822679331\n",
      "1.4840006583501713\n",
      "1.4852186960761935\n",
      "1.4992001448803027\n",
      "1.483074708253202\n",
      "1.4887806571831415\n",
      "1.4862012667740254\n",
      "1.472838475281594\n",
      "1.4668115571977844\n",
      "1.4713278754655212\n",
      "1.4653378003303557\n",
      "1.459036435205372\n",
      "1.4718083920242988\n",
      "1.462957751167842\n",
      "1.4643677419130248\n",
      "1.4773774313105312\n",
      "1.4706484833742304\n",
      "1.4683186838882307\n",
      "1.427616795504748\n",
      "1.4259216366007066\n",
      "1.4292483977811452\n",
      "1.4410157453500374\n",
      "1.4264798596427117\n",
      "1.4294196498620764\n",
      "1.4327828710457358\n",
      "1.4219022008205633\n",
      "1.4195507942092394\n",
      "1.456923131372284\n",
      "1.4229963273620279\n",
      "1.396470724538578\n",
      "1.391970284970526\n",
      "1.3922580342746842\n",
      "1.4098710944346928\n",
      "1.3944783892560262\n",
      "1.3868823568331896\n",
      "1.4058728778026897\n",
      "1.3854046567730973\n",
      "1.3898538041950557\n",
      "1.3572938556633583\n",
      "1.3483295517485723\n",
      "1.3433294616556237\n",
      "1.33897676199504\n",
      "1.3388418404802729\n",
      "1.3384030427014508\n",
      "1.3422821177444453\n",
      "1.3479360461067342\n",
      "1.3499162877762576\n",
      "1.3496023700217477\n",
      "1.3346657620014404\n",
      "1.3205409963822192\n",
      "1.3262724025691903\n",
      "1.3232103671784943\n",
      "1.3163125527024628\n",
      "1.316926351188194\n",
      "1.3235066481760123\n",
      "1.317615838291268\n",
      "1.3341990982656564\n",
      "1.3222067440250873\n",
      "1.3322883334604603\n",
      "1.3083363076956422\n",
      "1.2907535430038541\n",
      "1.301604610403297\n",
      "1.2925284654359386\n",
      "1.307942495056014\n",
      "1.2898975885169623\n",
      "1.2934788382200537\n",
      "1.2934928735320612\n",
      "1.3153484693979423\n",
      "1.2884112001369632\n",
      "1.288519354519634\n",
      "1.2803541371783904\n",
      "1.2787930989729208\n",
      "1.27965650929265\n",
      "1.2788530244751148\n",
      "1.2777559525935143\n",
      "1.281028751484711\n",
      "1.275467419706121\n",
      "1.2869932781662377\n",
      "1.2733720903100636\n",
      "1.267448066918799\n",
      "1.2582890740350487\n",
      "1.2545082761042905\n",
      "1.254945360280789\n",
      "1.2543084698663531\n",
      "1.2538307408474678\n",
      "1.256406145499587\n",
      "1.2548045260185212\n",
      "1.2580289245932537\n",
      "1.252763298499874\n",
      "1.2452734343677359\n",
      "1.2396237202628264\n",
      "1.2401489148073819\n",
      "1.2401621542149828\n",
      "1.2407454849825856\n",
      "1.2393863366685796\n",
      "1.2385709503475189\n",
      "1.2404451332861317\n",
      "1.2392856086858115\n",
      "1.244591936982416\n",
      "1.2294156608260969\n",
      "1.2245750307003302\n",
      "1.224545792218744\n",
      "1.2254077069118898\n",
      "1.2251161375146489\n",
      "1.224940061565161\n",
      "1.2233917893917847\n",
      "1.2215850222980342\n",
      "1.2271712394533922\n",
      "1.2199220718393586\n",
      "1.2166885787412713\n",
      "1.211682865162227\n",
      "1.2099425651143862\n",
      "1.2105427447032662\n",
      "1.2104258972829631\n",
      "1.2140196411782642\n",
      "1.206562398984508\n",
      "1.2067039640563577\n",
      "1.2062509140466224\n",
      "1.2055541840724966\n",
      "1.198069370408995\n",
      "1.189299062731149\n",
      "1.1893797802214272\n",
      "1.1898738619329414\n",
      "1.1876879341100466\n",
      "1.1865229786366547\n",
      "1.1914736054672463\n",
      "1.1860954718730279\n",
      "1.1783947413104532\n",
      "1.1750175881300644\n",
      "1.171286572221392\n",
      "1.172056251506065\n",
      "1.1714875063903316\n",
      "1.169310640996486\n",
      "1.169679069154494\n",
      "1.1735766376822665\n",
      "1.1694892027026755\n",
      "1.164262466000819\n",
      "1.1710956326928865\n",
      "1.158641376987845\n",
      "1.150615438957469\n",
      "1.1489792982046325\n",
      "1.1502712551039111\n",
      "1.1499317659625592\n",
      "1.1493071945437792\n",
      "1.1507885072841417\n",
      "1.1510064990370197\n",
      "1.156957916632556\n",
      "1.150734014839041\n",
      "1.1415910823628366\n",
      "1.1329955741486932\n",
      "1.1263709977186498\n",
      "1.1163567943125752\n",
      "1.1135590129955606\n",
      "1.1138750416646153\n",
      "1.1127648548969553\n",
      "1.1090639231787942\n",
      "1.1074437111637758\n",
      "1.1030750269304026\n",
      "1.092496371314104\n",
      "1.0837693090257943\n",
      "1.0753610490638037\n",
      "1.0661407025121348\n",
      "1.0693809371221055\n",
      "1.0666314611654408\n",
      "1.0573846028257963\n",
      "1.0517080292641812\n",
      "1.039853828241924\n",
      "1.0319770520937004\n",
      "1.0225339059369936\n",
      "1.017484471019135\n",
      "1.0111999144931092\n",
      "1.0122767425934103\n",
      "1.0109947933571308\n",
      "1.0109473777914042\n",
      "1.011144412925231\n",
      "1.0111772700843666\n",
      "1.0057887547176079\n",
      "1.001210614222905\n",
      "0.9921088947345059\n",
      "0.987888876598403\n",
      "0.9875075547806934\n",
      "0.9763183717785577\n",
      "0.9685413248267274\n",
      "0.9579052702492719\n",
      "0.9563503214342655\n",
      "0.9535553672913748\n",
      "0.9449103838467596\n",
      "0.9383878003921339\n",
      "0.9370764768068122\n",
      "0.9360706650464994\n",
      "0.9393470144321743\n",
      "0.9362008191497838\n",
      "0.9376824951154908\n",
      "0.9345532442749349\n",
      "0.932461222927318\n",
      "0.9329786198735825\n",
      "0.9248502522970428\n",
      "0.9214607701047215\n",
      "0.9189387005508124\n",
      "0.9083462907535732\n",
      "0.8993551600698819\n",
      "0.89346415626953\n",
      "0.8880174969557635\n",
      "0.8888201399865605\n",
      "0.8882230928880337\n",
      "0.8790688327296036\n",
      "0.8766975328706477\n",
      "0.8755077714616931\n",
      "0.8765011759795224\n",
      "0.8734225525189053\n",
      "0.8724098995438585\n",
      "0.8682155286111713\n",
      "0.8693225723889905\n",
      "0.8677282135514399\n",
      "0.8627208310802893\n",
      "0.8550019244468902\n",
      "0.8469747737538197\n",
      "0.8411331317432662\n",
      "0.8370320024249336\n",
      "0.831624754786226\n",
      "0.8282994386030045\n",
      "0.8233524990409091\n",
      "0.8180078015932053\n",
      "0.8143746499861766\n",
      "0.8177758830016856\n",
      "0.8152287114466275\n",
      "0.8147787547666954\n",
      "0.8149400949787042\n",
      "0.8131469120806725\n",
      "0.8147338645950327\n",
      "0.8116074688864581\n",
      "0.8125895739475864\n",
      "0.8045876941224619\n",
      "0.800423097865513\n",
      "0.7997602883761227\n",
      "0.8007002571395456\n",
      "0.7954309569196284\n",
      "0.7947182452060099\n",
      "0.7923523437777624\n",
      "0.7873921985017821\n",
      "0.7855770075852769\n",
      "0.7805757633864084\n",
      "0.7789763095941448\n",
      "0.7800591111433748\n",
      "0.7772626562542303\n",
      "0.7777477287098828\n",
      "0.7737689893317601\n",
      "0.7745742754266491\n",
      "0.7735624531424231\n",
      "0.769202589417757\n",
      "0.7689856628504765\n",
      "0.7683694897411945\n",
      "0.7669710440101122\n",
      "0.7670824440613658\n",
      "0.7643525377520396\n",
      "0.7616095873096699\n",
      "0.7609861942420346\n",
      "0.7615318202080177\n",
      "0.7592742878881256\n",
      "0.7568471955646462\n",
      "0.7554747375312216\n",
      "0.7555447062735272\n",
      "0.7555533410904125\n",
      "0.7563951950455767\n",
      "0.7535949501995491\n",
      "0.7549686546598313\n",
      "0.7537019863588948\n",
      "0.7520146340781115\n",
      "0.7506808194025847\n",
      "0.7515030568848918\n",
      "0.7508501019546361\n",
      "0.7505158526294383\n",
      "0.7506260960365068\n",
      "0.7509678686283051\n",
      "0.7502282747781688\n",
      "0.7512049156444078\n",
      "0.7501486405401513\n",
      "0.7507848571537008\n",
      "0.7507050595156685\n",
      "0.7503223168938242\n",
      "0.750489311595546\n",
      "0.7517208768931927\n",
      "0.750726767315385\n",
      "0.7492849219986487\n",
      "0.7500684671881964\n",
      "0.749374181373464\n",
      "0.7495657854372207\n",
      "0.7492939385259907\n",
      "0.7489638999137107\n",
      "0.7490096966798432\n",
      "0.7485594886529787\n",
      "0.7487584354199104\n",
      "0.7483459704592308\n",
      "0.7486733880096087\n",
      "0.7480666903240428\n",
      "0.7477936422839591\n",
      "0.7477971003601993\n",
      "0.7479083330630864\n",
      "0.7479313943467177\n",
      "0.7478964858901668\n",
      "0.7480754268563262\n",
      "0.7481894382173512\n",
      "0.7472261672529452\n",
      "0.7473809493779026\n",
      "0.7473233073232856\n",
      "0.7473731387762069\n",
      "0.747298295419698\n",
      "0.7473241877803793\n",
      "0.7471945998179799\n",
      "0.7473552301306563\n",
      "0.7473036156327559\n",
      "0.7470847540552318\n",
      "0.7472883857868313\n",
      "0.7471950901995958\n",
      "0.7471319323097781\n",
      "0.7474575644477501\n",
      "0.7471005521994223\n",
      "0.7469674625789681\n",
      "0.7474153973796221\n",
      "0.7470504320218737\n",
      "0.7473209331962702\n",
      "0.746819932534822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 0.35774276+1.22189288j, -0.86149197+1.10219513j],\n",
       "        [ 0.88575085-0.37516382j, -0.29658438+0.68993921j],\n",
       "        [-0.27069744-0.02968713j, -0.6219367 -0.11387994j],\n",
       "        [-0.78843526-0.30802891j, -0.25192007+0.96659435j]]),\n",
       " 0.21225)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_testing():\n",
    "    H_hat_vec = np.sqrt(1/2)*(np.random.randn(Nr*Nt*2))\n",
    "    BER_performance = 1\n",
    "\n",
    "    out = minimize(calculate_cost_function, x0=H_hat_vec, method=\"COBYLA\", options={'maxiter':500})\n",
    "\n",
    "    H_hat_vec = out.x\n",
    "\n",
    "    H_trained = H_hat_vec[0:Nr*Nt].reshape(Nr,Nt)+1j*H_hat_vec[Nr*Nt:2*Nr*Nt].reshape(Nr,Nt)\n",
    "    BER_performance = calculate_BER(H_trained)\n",
    "\n",
    "    return H_trained, BER_performance\n",
    "\n",
    "training_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4052545 +1.1364205j , -0.69477697+0.67637798j],\n",
       "       [ 0.51651071-0.71077743j, -0.25098806+0.67218313j],\n",
       "       [-0.25288016+0.6460937j , -0.20178888+0.01503802j],\n",
       "       [-0.87851524-0.21199186j, -0.59884798+0.84684823j]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD: 0.0108642578125\n"
     ]
    }
   ],
   "source": [
    "from sphere_decoding.sphereDecodingUseC import sphere_decoding_BER\n",
    "bits_sequence, x_sequence, y_sequence = generate_data(Nr,Nt,20,1024,H)\n",
    "a = sphere_decoding_BER(H, y_sequence, bits_sequence, 1)\n",
    "print(\"SD: \"+str(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
